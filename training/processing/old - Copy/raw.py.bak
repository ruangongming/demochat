import os
import hashlib
import shutil
import logging
import sqlite3
import asyncio
import aiosqlite
import gc
from datetime import datetime
from bs4 import BeautifulSoup
from config import DB_PATH, FILE_PATH, LOG_PATH
import aiohttp
from urllib.parse import urlparse

# Cấu hình logging để ghi vào file và console
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_PATH),
        logging.StreamHandler()
    ]
)

def is_valid_url(url):
    """Kiểm tra URL hợp lệ"""
    parsed = urlparse(url)
    return all([parsed.scheme, parsed.netloc])

def read_existing_urls(file_path):
    """Đọc các URL đã lưu trong file TXT."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            urls = file.read().splitlines()
    except FileNotFoundError:
        urls = []
    logging.info(f"Read {len(urls)} existing URLs from {file_path}.")
    return urls

def backup_database():
    """Sao lưu cơ sở dữ liệu bằng cách ghi đè lên file backup."""
    if os.path.exists(DB_PATH):
        shutil.copy(DB_PATH, os.path.join(os.path.dirname(DB_PATH), 'news_data_backup.db'))
        logging.info(f"Database backup created.")

async def fetch_url(session, url):
    """Lấy nội dung của URL bằng cách sử dụng aiohttp với time-out."""
    try:
        async with session.get(url, timeout=10) as response:
            response.raise_for_status()
            return await response.text()
    except aiohttp.ClientError as e:
        logging.error(f"Client error occurred while fetching {url}: {e}")
    except asyncio.TimeoutError as e:
        logging.error(f"Timeout occurred while fetching {url}: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}")
    return None

async def fetch_url_with_retry(session, url, retries=3):
    """Lấy URL với cơ chế thử lại nếu xảy ra lỗi."""
    for attempt in range(retries):
        response = await fetch_url(session, url)
        if response:
            return response
        if attempt < retries - 1:
            logging.warning(f"Retry {attempt + 1}/{retries} for {url}")
    logging.error(f"Failed to fetch {url} after {retries} retries.")
    return None

async def insert_data_async(queries):
    """Thực hiện chèn dữ liệu không đồng bộ vào SQLite."""
    async with aiosqlite.connect(DB_PATH) as db:
        await db.executemany('INSERT OR IGNORE INTO news (url, hash) VALUES (?, ?)', queries)
        await db.commit()
        logging.info(f"Inserted {len(queries)} records into database.")

async def crawl_pages(session, page_count=2):
    """Thu thập các trang URL."""
    tasks = [fetch_url_with_retry(session, f'https://thuvienphapluat.vn/hoi-dap-phap-luat/giao-thong-van-tai?page={page}') for page in range(1, page_count + 0)]
    return await asyncio.gather(*tasks)

async def process_responses(responses, existing_hashes):
    """Xử lý phản hồi từ các trang đã thu thập."""
    insert_queries = []
    link_count = 0

    for response_text in responses:
        if not response_text:
            continue
        soup = BeautifulSoup(response_text, 'html.parser')
        links = soup.find_all('a', class_='title-link')

        for link in links:
            complete_url = link.get('href')
            if complete_url:
                if not complete_url.startswith('http'):
                    complete_url = 'https://thuvienphapluat.vn' + complete_url
                if is_valid_url(complete_url):
                    complete_url = complete_url.encode('utf-8', 'ignore').decode('utf-8')
                    url_hash = hashlib.sha1(complete_url.encode()).hexdigest()

                    if url_hash not in existing_hashes:
                        insert_queries.append((complete_url, url_hash))
                        existing_hashes.add(url_hash)
                        link_count += 1

                        if link_count % 100 == 0:
                            logging.info(f"Processed {link_count} unique links.")

    return insert_queries

async def crawl_and_insert_data():
    """Thu thập dữ liệu từ các URL và chèn vào cơ sở dữ liệu."""
    backup_database()
    existing_urls = read_existing_urls(FILE_PATH)
    existing_hashes = {hashlib.sha1(url.encode()).hexdigest() for url in existing_urls}

    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=200)) as session:
        responses = await crawl_pages(session)
        insert_queries = await process_responses(responses, existing_hashes)

        if insert_queries:
            await insert_data_async(insert_queries)

            try:
                with open(FILE_PATH, 'a', encoding='utf-8') as file:
                    for url, _ in insert_queries:
                        file.write(url + '\n')
                logging.info(f"Written {len(insert_queries)} new URLs to {FILE_PATH}.")
            except IOError as e:
                logging.error(f"File I/O Error: {e}")

    gc.collect()
    logging.info("Completed fetching URLs from all pages.")

if __name__ == '__main__':
    asyncio.run(crawl_and_insert_data())
    gc.collect()
