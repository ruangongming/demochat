import gc
import sys
import os
import codecs
from newspaper import Article
import validators
from urllib.parse import urlparse
import re
import sqlite3
from datetime import datetime
import shutil
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from config import DB_PATH, FILE_PATH, OUTPUT_DIR, LOG_PATH
from bs4 import BeautifulSoup
import asyncio
import aiohttp
from tenacity import retry, stop_after_attempt, wait_fixed

# Thiết lập lại stdout và stderr về 'utf-8' để xử lý Unicode đúng cách
sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())

# Cấu hình logging để ghi vào file và console, mức độ log là INFO
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_PATH, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# Tạo một khóa để đảm bảo quá trình ghi vào SQLite được an toàn khi đa luồng
db_lock = Lock()


def read_existing_urls(file_path):
    """Đọc các URL đã lưu trong file TXT."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            urls = file.read().splitlines()
    except FileNotFoundError:
        urls = []
    return urls


def backup_database():
    """Sao lưu cơ sở dữ liệu bằng cách ghi đè lên file backup."""
    if os.path.exists(DB_PATH):
        shutil.copy(DB_PATH, os.path.join(os.path.dirname(DB_PATH), 'news_data_backup.db'))


def initialize_database():
    """Kiểm tra và khởi tạo cơ sở dữ liệu SQLite."""
    if not os.path.exists(DB_PATH):
        logging.info("Database not found, initializing...")
        from db import initialize_database
        initialize_database()
    backup_database()


def is_url(url):
    """Kiểm tra tính hợp lệ của URL."""
    return validators.url(url)


def is_youtube_url(url):
    """Kiểm tra URL có phải YouTube không."""
    return 'youtube.com' in url or 'youtu.be' in url


def fix_spacing(text):
    """
    Hàm tổng quát để đảm bảo các từ luôn được cách nhau bằng dấu cách,
    bao gồm cả trường hợp số dính vào chữ và dấu câu không có khoảng cách.
    """
    # Thêm dấu cách giữa chữ viết thường và chữ viết hoa không có khoảng cách
    text = re.sub(
        r'([a-záàảãạâấầẩẫậăắằẳẵặđéèẻẽẹêếềểễệíìỉĩịôồốổỗộơờớởỡợúùủũụưứừửữựýỳỷỹỵ])([A-ZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶĐÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÔỒỐỔỖỘƠỜỚỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴ])',
        r'\1 \2', text)

    # Thêm dấu cách giữa số và chữ
    text = re.sub(r'(\d)([a-zA-Z])', r'\1 \2', text)  # "10Luật" -> "10 Luật"
    text = re.sub(r'([a-zA-Z])(\d)', r'\1 \2', text)  # "Luật10" -> "Luật 10"

    # Thêm dấu cách giữa ký tự viết hoa và các ký tự liền kề không hợp lệ, ví dụ: "TT-BCAquy" -> "TT-BCA quy"
    text = re.sub(r'([A-Z]{2,})([a-z])', r'\1 \2', text)

    # Thêm dấu cách sau dấu câu nếu bị thiếu khoảng cách
    text = re.sub(r'([.,:;!?])([^\s])', r'\1 \2', text)  # Thêm khoảng cách sau dấu câu

    # Loại bỏ các chuỗi thừa như dấu câu lặp, hoặc các ký tự thừa
    text = re.sub(r'\[\.\s*\.\s*\.\s*\]', '', text)  # Loại bỏ "[. .. ]" hoặc "[...]"
    text = re.sub(r'\s+', ' ', text)  # Loại bỏ khoảng trắng thừa

    return text


@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))
async def fetch_page(session, url):
    """Thực hiện yêu cầu HTTP không đồng bộ với cơ chế retry."""
    async with session.get(url, timeout=10) as response:
        response.raise_for_status()
        return await response.text()


async def crawl(url):
    """Thu thập dữ liệu từ URL không đồng bộ và trích xuất các bài viết nhỏ."""
    result = {
        'url': url,
        'articles': []
    }

    async with aiohttp.ClientSession() as session:
        try:
            html = await fetch_page(session, url)
            soup = BeautifulSoup(html, 'html.parser')

            # Lấy tất cả các thẻ h2 và phần nội dung dưới mỗi thẻ
            titles = soup.find_all('h2')
            for idx, title in enumerate(titles):
                article_content = []
                next_tag = title.find_next_sibling()

                # Thu thập tất cả các thẻ p, blockquote sau h2 cho đến thẻ h2 tiếp theo
                while next_tag and next_tag.name not in ['h2', 'h1']:
                    if next_tag.name in ['p', 'blockquote']:
                        # Làm sạch nội dung, đảm bảo các từ cách nhau bằng dấu cách và không dính số/chữ
                        clean_text = fix_spacing(next_tag.get_text(strip=True))
                        article_content.append(clean_text)
                    next_tag = next_tag.find_next_sibling()

                if article_content:
                    result['articles'].append({
                        'title': title.get_text(strip=True),
                        'content': '\n'.join(article_content),
                        'index': idx + 1  # Đánh số từ 1 đến n
                    })

        except Exception as e:
            logging.error(f"Lỗi khi xử lý URL {url}: {e}")

    return result


def save_articles_to_file(data, output_dir, log_every=10):
    """Lưu từng bài viết nhỏ vào file với tiêu đề đầy đủ."""
    for count, article in enumerate(data['articles'], start=1):
        # Sử dụng tiêu đề đầy đủ làm tên file, loại bỏ các ký tự không hợp lệ
        file_name = re.sub(r'[\\/*?:"<>|]', "_", article['title'])
        file_path = os.path.join(output_dir, f"{file_name}.txt")

        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(f"Tiêu đề: {article['title']}\n\n")
            file.write("Nội dung:\n")
            file.write(article['content'])

        if count % log_every == 0:
            logging.info(f"Đã lưu bài viết vào file: {file_path}")


def insert_into_db(data):
    """Chèn từng bài viết nhỏ từ một URL vào cơ sở dữ liệu."""
    articles = data['articles']
    if not articles:
        logging.warning("No valid articles to insert into database.")
        return

    with db_lock:
        try:
            with sqlite3.connect(DB_PATH) as conn:
                cursor = conn.cursor()
                cursor.executemany('''
                    INSERT OR REPLACE INTO news (url, title, content) VALUES (?, ?, ?)
                ''', [(data['url'], article['title'], article['content']) for article in articles])
                conn.commit()
        except sqlite3.Error as e:
            logging.error(f"SQLite Error: {e}")


async def process_url(url, count):
    """Xử lý URL và lưu từng bài viết nhỏ."""
    if is_youtube_url(url):
        return None

    result = await crawl(url)
    if result and result['articles']:
        # Lưu từng bài viết nhỏ vào file với tiêu đề đầy đủ
        save_articles_to_file(result, OUTPUT_DIR, count)
        # Chèn các bài viết nhỏ vào cơ sở dữ liệu
        insert_into_db(result)
        return True
    return None


async def main():
    initialize_database()

    # Đọc các URL từ file TXT
    urls = read_existing_urls(FILE_PATH)

    # Tạo một danh sách các task không đồng bộ
    tasks = [process_url(url, idx) for idx, url in enumerate(urls, start=1)]

    # Chạy các task không đồng bộ
    await asyncio.gather(*tasks)


if __name__ == '__main__':
    try:
        asyncio.run(main())
        gc.collect()  # Giải phóng bộ nhớ
    except Exception as e:
        logging.error(f"Error: {e}")
    finally:
        gc.collect()  # Giải phóng bộ nhớ
